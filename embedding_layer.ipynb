{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Layers are a powerful feature of Keras that allow additional information to be automatically inserted into your neural network. In the previous section you saw that Word2Vec can expand words to a 300 dimension vector. An embedding layer would allow you to automatically insert these 300-dimension vectors in the place of word-indexes.\n",
    "\n",
    "Embedding layers are often used with Natural Language Processing (NLP); however, they can be used in any instance where you wish to insert a larger vector in the place of an index value. In some ways you can think of an embedding layer as dimension expansion. However, the hope is that these additional dimensions will provide more information to the model and provide a better score.\n",
    "\n",
    ">Simple Embedding Layer Example\n",
    "\n",
    "**input_dim =** How large is the vocabulary? How many categories are you encoding. This is the number of items in your \"lookup table\".\n",
    "\n",
    "**output_dim =** How many numbers in the vector that you wish to return.\n",
    "\n",
    "**input_length =** How many items are in the input feature vector that you need to transform?\n",
    "\n",
    "Now we create one that has a vocabulary size of 10, will reduce those values between 0-9 to 4 number vectors. Each feature vector coming in will have 2 such features. This neural network does nothing more than pass the embedding on to the output. But it does let us see what the embedding is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=10,output_dim=4,input_length=2)\n",
    "model.add(embedding_layer)\n",
    "model.compile('adam','mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2, 4)              40        \n",
      "=================================================================\n",
      "Total params: 40\n",
      "Trainable params: 40\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets query the neural network with 2 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_data = np.array([[1,2]])\n",
    "pred = model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "[[[-0.02423859  0.0030961  -0.02844035  0.04632447]\n",
      "  [-0.01272523  0.00831414 -0.03736687 -0.01269807]]]\n"
     ]
    }
   ],
   "source": [
    "print(input_data.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.71123818e-03, -2.18733437e-02, -1.68860070e-02,\n",
       "          4.93349172e-02],\n",
       "        [-2.42385864e-02,  3.09610367e-03, -2.84403451e-02,\n",
       "          4.63244654e-02],\n",
       "        [-1.27252340e-02,  8.31414387e-03, -3.73668671e-02,\n",
       "         -1.26980655e-02],\n",
       "        [ 2.17055343e-02, -1.42876133e-02,  4.78557497e-03,\n",
       "         -1.17921829e-03],\n",
       "        [-2.28900667e-02,  3.40420939e-02,  8.22656229e-03,\n",
       "         -4.20046076e-02],\n",
       "        [-1.84133053e-02,  3.02759893e-02, -1.11472718e-02,\n",
       "         -1.33972764e-02],\n",
       "        [-1.52776018e-02, -1.23007186e-02,  3.94559391e-02,\n",
       "          3.01836617e-02],\n",
       "        [-4.97437119e-02, -2.79242396e-02,  2.88332365e-02,\n",
       "         -4.12040241e-02],\n",
       "        [ 4.71055396e-02, -4.60229293e-02,  3.77143063e-02,\n",
       "          4.03508879e-02],\n",
       "        [-3.92954946e-02,  4.75839414e-02,  3.64915244e-02,\n",
       "         -9.89325345e-05]], dtype=float32)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


